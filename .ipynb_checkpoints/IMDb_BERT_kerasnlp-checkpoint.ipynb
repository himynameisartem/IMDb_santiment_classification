{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# BERT-модель для анализа тональности IMDB на Keras с использованием keras-nlp\n",
        "\n",
        "# Если нет keras-nlp и datasets, раскомментируй и запусти строку ниже ОДИН раз:\n",
        "# !pip install -U keras-nlp datasets\n",
        "\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "import keras_nlp\n",
        "from datasets import load_dataset\n",
        "\n",
        "print(\"TensorFlow:\", tf.__version__)\n",
        "print(\"Keras:\", keras.__version__)\n",
        "print(\"keras-nlp:\", keras_nlp.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Загрузка текстового IMDB через HuggingFace datasets\n",
        "\n",
        "raw_datasets = load_dataset(\"imdb\")\n",
        "\n",
        "train_ds = raw_datasets[\"train\"]\n",
        "test_ds = raw_datasets[\"test\"]\n",
        "\n",
        "# Выделяем валидацию из train (20%)\n",
        "train_valid = train_ds.train_test_split(test_size=0.2, seed=42)\n",
        "train_ds = train_valid[\"train\"]\n",
        "val_ds = train_valid[\"test\"]\n",
        "\n",
        "print(\"Train:\", len(train_ds))\n",
        "print(\"Val:\", len(val_ds))\n",
        "print(\"Test:\", len(test_ds))\n",
        "print(\"Пример текста:\\n\", train_ds[0][\"text\"][:300])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. Инициализация готового BERT-классификатора из keras-nlp\n",
        "\n",
        "# Используем маленький пресет, чтобы обучение в ноутбуке было посильным.\n",
        "# Можно заменить на \"bert_base_en_uncased\" если есть GPU и терпение.\n",
        "\n",
        "preset = \"bert_tiny_en_uncased\"\n",
        "\n",
        "classifier = keras_nlp.models.BertClassifier.from_preset(\n",
        "    preset,\n",
        "    num_classes=2,   # позитив/негатив\n",
        ")\n",
        "\n",
        "# Замораживаем часть слоёв (по желанию) для более стабильного обучения\n",
        "for layer in classifier.backbone.layers[:-2]:  # оставим несколько верхних обучаемыми\n",
        "    layer.trainable = False\n",
        "\n",
        "classifier.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=3e-5),\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "\n",
        "classifier.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3. Превращение HuggingFace datasets в tf.data.Dataset для Keras\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "# keras-nlp BERT-классификатор умеет сам токенизировать строки,\n",
        "# поэтому в tf.data мы передаём просто поле \"text\".\n",
        "\n",
        "train_tf = train_ds.to_tf_dataset(\n",
        "    columns=\"text\",          # вход: строка с текстом\n",
        "    label_cols=\"label\",      # метка 0/1\n",
        "    shuffle=True,\n",
        "    batch_size=batch_size,\n",
        ")\n",
        "\n",
        "val_tf = val_ds.to_tf_dataset(\n",
        "    columns=\"text\",\n",
        "    label_cols=\"label\",\n",
        "    shuffle=False,\n",
        "    batch_size=batch_size,\n",
        ")\n",
        "\n",
        "test_tf = test_ds.to_tf_dataset(\n",
        "    columns=\"text\",\n",
        "    label_cols=\"label\",\n",
        "    shuffle=False,\n",
        "    batch_size=batch_size,\n",
        ")\n",
        "\n",
        "for batch in train_tf.take(1):\n",
        "    x_batch, y_batch = batch\n",
        "    print(type(x_batch), x_batch.shape, y_batch.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4. Обучение BERT-классификатора\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.EarlyStopping(\n",
        "        monitor=\"val_accuracy\",\n",
        "        patience=2,\n",
        "        restore_best_weights=True,\n",
        "        verbose=1,\n",
        "    )\n",
        "]\n",
        "\n",
        "epochs = 3  # можно увеличить до 4–5 при наличии времени/ресурсов\n",
        "\n",
        "history = classifier.fit(\n",
        "    train_tf,\n",
        "    validation_data=val_tf,\n",
        "    epochs=epochs,\n",
        "    callbacks=callbacks,\n",
        ")\n",
        "\n",
        "print(\"Лучшая точность на валидации:\", max(history.history[\"val_accuracy\"]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5. Оценка на валидации и тесте\n",
        "\n",
        "print(\"\\nОценка на валидационной выборке:\")\n",
        "val_loss, val_acc = classifier.evaluate(val_tf)\n",
        "print(f\"Val accuracy: {val_acc:.4f} ({val_acc*100:.2f}%)\")\n",
        "\n",
        "print(\"\\nОценка на тестовой выборке:\")\n",
        "test_loss, test_acc = classifier.evaluate(test_tf)\n",
        "print(f\"Test accuracy: {test_acc:.4f} ({test_acc*100:.2f}%)\")\n",
        "\n",
        "print(\"\\nИтог:\")\n",
        "print(f\"Валидация: {val_acc*100:.2f}%\")\n",
        "print(f\"Тест: {test_acc*100:.2f}%\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
