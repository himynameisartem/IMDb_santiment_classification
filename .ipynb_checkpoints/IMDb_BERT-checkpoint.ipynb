{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# BERT-модель для анализа тональности на датасете IMDB\n",
        "# Вариант с использованием HuggingFace `datasets` и `transformers` + Keras (TFBertForSequenceClassification)\n",
        "\n",
        "# Если нужны зависимости, раскомментируй строку ниже и запусти один раз\n",
        "# !pip install -U datasets transformers\n",
        "\n",
        "import tensorflow as tf\n",
        "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
        "from datasets import load_dataset\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(tf.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Загрузка текстового датасета IMDB\n",
        "\n",
        "# Загружаем стандартный текстовый IMDB из HuggingFace\n",
        "# train: 25 000 отзывов, test: 25 000 отзывов\n",
        "raw_datasets = load_dataset(\"imdb\")\n",
        "\n",
        "train_ds = raw_datasets[\"train\"]\n",
        "test_ds = raw_datasets[\"test\"]\n",
        "\n",
        "print(train_ds)\n",
        "print(test_ds)\n",
        "\n",
        "# Выделим валидационную выборку из train (например, 20%)\n",
        "train_valid = train_ds.train_test_split(test_size=0.2, seed=42)\n",
        "train_ds = train_valid[\"train\"]\n",
        "val_ds = train_valid[\"test\"]\n",
        "\n",
        "print(\"\\nSizes:\")\n",
        "print(\"Train:\", len(train_ds))\n",
        "print(\"Val:\", len(val_ds))\n",
        "print(\"Test:\", len(test_ds))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. Инициализация токенизатора BERT и настройка параметров\n",
        "\n",
        "model_name = \"bert-base-uncased\"  # можно заменить на bert-tiny/bert-small для ускорения\n",
        "max_length = 256                   # максимальная длина последовательности для BERT\n",
        "\n",
        "# Загружаем токенизатор\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "\n",
        "print(\"Пример токенизации:\")\n",
        "example_text = train_ds[0][\"text\"][:200]\n",
        "print(example_text)\n",
        "print(tokenizer(example_text))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3. Токенизация датасета для BERT\n",
        "\n",
        "# Функция для токенизации батчей из HuggingFace Datasets\n",
        "def tokenize_batch(batch):\n",
        "    return tokenizer(\n",
        "        batch[\"text\"],\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "    )\n",
        "\n",
        "# Применяем токенизацию к train/val/test\n",
        "tokenized_train = train_ds.map(tokenize_batch, batched=True)\n",
        "tokenized_val = val_ds.map(tokenize_batch, batched=True)\n",
        "tokenized_test = test_ds.map(tokenize_batch, batched=True)\n",
        "\n",
        "# Оставляем только нужные поля\n",
        "tokenized_train = tokenized_train.remove_columns([col for col in tokenized_train.column_names if col not in [\"input_ids\", \"attention_mask\", \"label\"]])\n",
        "tokenized_val = tokenized_val.remove_columns([col for col in tokenized_val.column_names if col not in [\"input_ids\", \"attention_mask\", \"label\"]])\n",
        "tokenized_test = tokenized_test.remove_columns([col for col in tokenized_test.column_names if col not in [\"input_ids\", \"attention_mask\", \"label\"]])\n",
        "\n",
        "print(tokenized_train[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4. Преобразование в tf.data.Dataset для обучения в Keras\n",
        "\n",
        "# Настройки батча\n",
        "batch_size = 16\n",
        "\n",
        "# Переводим HuggingFace Datasets в tf.data.Dataset\n",
        "def hf_to_tf_dataset(tokenized_ds):\n",
        "    tokenized_ds.set_format(type=\"tensorflow\")\n",
        "    features = {\n",
        "        \"input_ids\": tokenized_ds[\"input_ids\"],\n",
        "        \"attention_mask\": tokenized_ds[\"attention_mask\"],\n",
        "    }\n",
        "    labels = tokenized_ds[\"label\"]\n",
        "    tf_ds = tf.data.Dataset.from_tensor_slices((features, labels))\n",
        "    return tf_ds\n",
        "\n",
        "train_tf = hf_to_tf_dataset(tokenized_train).shuffle(10000).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "val_tf = hf_to_tf_dataset(tokenized_val).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "test_tf = hf_to_tf_dataset(tokenized_test).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "for batch in train_tf.take(1):\n",
        "    x_batch, y_batch = batch\n",
        "    print(x_batch[\"input_ids\"].shape, x_batch[\"attention_mask\"].shape, y_batch.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5. Создание и компиляция BERT-модели в Keras\n",
        "\n",
        "# Загружаем предобученный BERT как Keras-модель (TFBertForSequenceClassification)\n",
        "model = TFBertForSequenceClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=2\n",
        ")\n",
        "\n",
        "# Оптимизатор и функция потерь для задачи бинарной классификации\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "metrics = [\"accuracy\"]\n",
        "\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 6. Обучение модели BERT на IMDB\n",
        "\n",
        "# Callback'и: ранняя остановка и сохранение лучшей модели\n",
        "checkpoint_path = \"bert_imdb_best\"\n",
        "\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.EarlyStopping(\n",
        "        monitor=\"val_accuracy\",\n",
        "        patience=2,\n",
        "        restore_best_weights=True,\n",
        "        verbose=1,\n",
        "    ),\n",
        "]\n",
        "\n",
        "epochs = 3\n",
        "\n",
        "history = model.fit(\n",
        "    train_tf,\n",
        "    validation_data=val_tf,\n",
        "    epochs=epochs,\n",
        "    callbacks=callbacks,\n",
        ")\n",
        "\n",
        "print(\"Лучшая точность на валидации:\", max(history.history[\"val_accuracy\"]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 7. Графики обучения (loss и accuracy)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "epochs_range = range(1, len(history.history[\"loss\"]) + 1)\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, history.history[\"loss\"], \"bo-\", label=\"Train loss\")\n",
        "plt.plot(epochs_range, history.history[\"val_loss\"], \"ro-\", label=\"Val loss\")\n",
        "plt.title(\"Training and validation loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, history.history[\"accuracy\"], \"bo-\", label=\"Train acc\")\n",
        "plt.plot(epochs_range, history.history[\"val_accuracy\"], \"ro-\", label=\"Val acc\")\n",
        "plt.title(\"Training and validation accuracy\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 8. Оценка модели на валидации и тесте\n",
        "\n",
        "print(\"\\nОценка на валидационной выборке:\")\n",
        "val_loss, val_acc = model.evaluate(val_tf)\n",
        "print(f\"Val accuracy: {val_acc:.4f} ({val_acc*100:.2f}%)\")\n",
        "\n",
        "print(\"\\nОценка на тестовой выборке:\")\n",
        "test_loss, test_acc = model.evaluate(test_tf)\n",
        "print(f\"Test accuracy: {test_acc:.4f} ({test_acc*100:.2f}%)\")\n",
        "\n",
        "print(\"\\nИтог:\")\n",
        "print(f\"Валидация: {val_acc*100:.2f}%\")\n",
        "print(f\"Тест: {test_acc*100:.2f}%\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
